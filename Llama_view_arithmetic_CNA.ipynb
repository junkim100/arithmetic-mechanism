{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8786cf7-f163-4cb6-a72d-7ef88fe25116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy, math\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "LAYER_NUM = 32\n",
    "HEAD_NUM = 32\n",
    "HEAD_DIM = 128\n",
    "HIDDEN_DIM = HEAD_NUM * HEAD_DIM\n",
    "torch.set_default_device(\"cuda\")\n",
    "zero_tensor = torch.tensor([0.0]*4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c894b5-dd62-42d1-891a-e08282cde52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_output(model_output):\n",
    "    all_pos_layer_input = []\n",
    "    all_pos_attn_output = []\n",
    "    all_pos_residual_output = []\n",
    "    all_pos_ffn_output = []\n",
    "    all_pos_layer_output = []\n",
    "    all_last_attn_subvalues = []\n",
    "    all_pos_coefficient_scores = []\n",
    "    all_attn_scores = []\n",
    "    for layer_i in range(LAYER_NUM):\n",
    "        cur_layer_input = model_output[layer_i][0]\n",
    "        cur_attn_output = model_output[layer_i][1]\n",
    "        cur_residual_output = model_output[layer_i][2]\n",
    "        cur_ffn_output = model_output[layer_i][3]\n",
    "        cur_layer_output = model_output[layer_i][4]\n",
    "        cur_last_attn_subvalues = model_output[layer_i][5]\n",
    "        cur_coefficient_scores = model_output[layer_i][6]\n",
    "        cur_attn_weights = model_output[layer_i][7]\n",
    "        all_pos_layer_input.append(cur_layer_input[0].tolist())\n",
    "        all_pos_attn_output.append(cur_attn_output[0].tolist())\n",
    "        all_pos_residual_output.append(cur_residual_output[0].tolist())\n",
    "        all_pos_ffn_output.append(cur_ffn_output[0].tolist())\n",
    "        all_pos_layer_output.append(cur_layer_output[0].tolist())\n",
    "        all_last_attn_subvalues.append(cur_last_attn_subvalues[0].tolist())\n",
    "        all_pos_coefficient_scores.append(cur_coefficient_scores[0].tolist())\n",
    "        all_attn_scores.append(cur_attn_weights)\n",
    "    return all_pos_layer_input, all_pos_attn_output, all_pos_residual_output, all_pos_ffn_output, \\\n",
    "           all_pos_layer_output, all_last_attn_subvalues, all_pos_coefficient_scores, all_attn_scores\n",
    "def get_fc2_params(model, layer_num):\n",
    "    return model.model.layers[layer_num].mlp.down_proj.weight.data\n",
    "def get_bsvalues(vector, model, final_var):\n",
    "    vector = vector * torch.rsqrt(final_var + 1e-6)\n",
    "    vector_rmsn = vector * model.model.norm.weight.data\n",
    "    vector_bsvalues = model.lm_head(vector_rmsn).data\n",
    "    return vector_bsvalues\n",
    "def get_layernorm_weight(model, layer_num):\n",
    "    return model.model.layers[layer_num].post_attention_layernorm.weight.data\n",
    "def get_prob(vector):\n",
    "    prob = torch.nn.Softmax(-1)(vector)\n",
    "    return prob\n",
    "def get_log_increase(model, all_ffn_subvalues, all_pos_residual_output, final_var, predict_index):\n",
    "    all_ffn_log_increase = []\n",
    "    for layer_i in range(LAYER_NUM):\n",
    "        cur_ffn_subvalues = all_ffn_subvalues[layer_i]\n",
    "        cur_residual = torch.tensor(all_pos_residual_output[layer_i][-1])\n",
    "        origin_prob_log = torch.log(get_prob(get_bsvalues(cur_residual, model, final_var))[predict_index])\n",
    "        cur_ffn_subvalues_plus = cur_ffn_subvalues + cur_residual\n",
    "        cur_ffn_subvalues_bsvalues = get_bsvalues(cur_ffn_subvalues_plus, model, final_var)\n",
    "        cur_ffn_subvalues_probs = get_prob(cur_ffn_subvalues_bsvalues)\n",
    "        cur_ffn_subvalues_probs = cur_ffn_subvalues_probs[:, predict_index]\n",
    "        cur_ffn_subvalues_probs_log = torch.log(cur_ffn_subvalues_probs)\n",
    "        cur_ffn_subvalues_probs_log_increase = cur_ffn_subvalues_probs_log - origin_prob_log\n",
    "        all_ffn_log_increase.append(cur_ffn_subvalues_probs_log_increase.tolist())\n",
    "    return all_ffn_log_increase\n",
    "def get_pos_vector(vector, pos_embed_var, model, layer_num):\n",
    "    vector = vector * torch.rsqrt(pos_embed_var + 1e-6)\n",
    "    vector_rmsn = vector * model.model.layers[layer_num].input_layernorm.weight.data\n",
    "    return vector_rmsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74ef03f-264b-49be-a089-e73e5b63e5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f15d0ae16b4ca69877c88b07f397db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please replace your own dir saving llama-7b model.\n",
    "#if you haven't downloaded it, you can try \"huggyllama/llama-7b\" to automatically download it from huggingface.\n",
    "modelname = \"../../scratch/save_models/llama-7b\" \n",
    "tokenizer = LlamaTokenizer.from_pretrained(modelname)\n",
    "model = LlamaForCausalLM.from_pretrained(modelname)\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8c3f1c-c7bb-48a9-8937-38c8cb692238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3+5= => ['8', '1', '6', '7', '2', '?', '', '3', '4', '5']\n",
      "['<s>', '', '3', '+', '5', '=']\n"
     ]
    }
   ],
   "source": [
    "#compute the final prediction of the input\n",
    "test_sentence = \"3+5=\"\n",
    "indexed_tokens = tokenizer.encode(test_sentence)\n",
    "tokens = [tokenizer.decode(x) for x in indexed_tokens]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "predicted_top10 = torch.argsort(predictions[0][-1], descending=True)[:10]\n",
    "predicted_text = [tokenizer.decode(x) for x in predicted_top10]\n",
    "print(test_sentence, \"=>\", predicted_text)\n",
    "all_pos_layer_input, all_pos_attn_output, all_pos_residual_output, all_pos_ffn_output, all_pos_layer_output, \\\n",
    "all_last_attn_subvalues, all_pos_coefficient_scores, all_attn_scores = transfer_output(outputs[1])\n",
    "final_var = torch.tensor(all_pos_layer_output[-1][-1]).pow(2).mean(-1, keepdim=True)\n",
    "pos_len = len(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061b5c41-3ddd-4e31-b0fd-1167707c29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29947 8\n",
      "prob:  0.3775949776172638\n"
     ]
    }
   ],
   "source": [
    "predict_index = predicted_top10[0].item()\n",
    "print(predict_index, tokenizer.decode(predict_index))\n",
    "cur_prob = get_prob(predictions[0][-1])[predict_index].item()\n",
    "print(\"prob: \", cur_prob)\n",
    "all_head_dict_prob, all_head_dict_logprob = {}, {}\n",
    "all_head_dict_prob[\"old\"] = cur_prob\n",
    "all_head_dict_logprob[\"old\"] = math.log(cur_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1bc03c-7f16-42cc-bd3d-a07e9c08d03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  -0.315999049693346 3+5= => ['6', '1', '7', '8', '?', '2', '5', '3', '4', '']\n"
     ]
    }
   ],
   "source": [
    "#the memory is broken when the most important head is zero-intervened\n",
    "all_attn_o_weights = []\n",
    "for layer_index in range(LAYER_NUM):\n",
    "    all_attn_o_weights.append(model.model.layers[layer_index].self_attn.o_proj.weight.data)\n",
    "model1 = copy.deepcopy(model)\n",
    "modify_heads = ['17_22']\n",
    "modify_heads_dict = {}\n",
    "for l_h in modify_heads:\n",
    "    l, h = l_h.split(\"_\")\n",
    "    if l not in modify_heads_dict:\n",
    "        modify_heads_dict[l] = []\n",
    "    modify_heads_dict[l].append(h)\n",
    "for l, hs in modify_heads_dict.items():\n",
    "    layer_index = int(l)\n",
    "    all_attn_o_weights_new = copy.deepcopy(all_attn_o_weights[layer_index])\n",
    "    for h in hs:\n",
    "        head_index = int(h)\n",
    "        start, end = HEAD_DIM*head_index, HEAD_DIM*head_index+HEAD_DIM\n",
    "        for neuron_index in range(start, end):\n",
    "            all_attn_o_weights_new[:, neuron_index] = zero_tensor\n",
    "    new_parameters = torch.nn.Parameter(all_attn_o_weights_new)\n",
    "    model1.model.layers[layer_index].self_attn.o_proj.weight = new_parameters\n",
    "    del all_attn_o_weights_new\n",
    "with torch.no_grad():\n",
    "    outputs1 = model1(tokens_tensor)\n",
    "    predictions1 = outputs1[0]\n",
    "cur_prob1 = get_prob(predictions1[0][-1])[predict_index].item()\n",
    "predicted_top10_1 = torch.argsort(predictions1[0][-1], descending=True)[:10]\n",
    "predicted_text1 = [tokenizer.decode(x) for x in predicted_top10_1]\n",
    "print(\"prob: \", cur_prob1-cur_prob, test_sentence, \"=>\", predicted_text1)\n",
    "all_pos_layer_input1, all_pos_attn_output1, all_pos_residual_output1, all_pos_ffn_output1, all_pos_layer_output1, \\\n",
    "all_last_attn_subvalues1, all_pos_coefficient_scores1, all_attn_scores1 = transfer_output(outputs1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a52ede0-afba-4697-91c0-e1c396b48745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('28_3696', 0.6964, 0.8229, 0.1265, -5.2503, -6.2005, -0.9502) ['8', 'eight', '⁸', 'VIII', 'huit', 'acht', '₈', '八', 'otto', 'e']\n",
      "('25_7164', 0.2327, 0.3097, 0.077, 6.3519, 8.4364, 2.0845) ['six', 'eight', 'acht', 'Four', 'Six', 'twelve', 'six', 'four', 'vier', 'four']\n",
      "('29_10957', 0.1384, 0.1464, 0.008, -6.2541, -6.6118, -0.3577) ['ві', 'desc', 'ги', 'rien', 'rach', 'Њ', 'usa', 'eight', 'Ott', 'Source']\n",
      "('19_5769', 0.1348, 0.2041, 0.0692, 2.5066, 3.7902, 1.2837) ['eight', 'VIII', '⁸', '8', 'III', 'huit', '₈', 'acht', 'XVIII', '<0x88>']\n",
      "('29_6563', 0.1322, 0.1812, 0.049, -1.5336, -2.0934, -0.5598) ['8', 'eight', '₈', '⁸', 'VIII', 'huit', 'otto', '7', 'acht', '₇']\n",
      "('23_2618', 0.1293, 0.2058, 0.0766, 2.426, 3.8565, 1.4305) ['eight', 'sevent', '8', 'huit', 'acht', '7', 'sept', '⁷', 'seven', 'VIII']\n",
      "('28_5475', 0.0965, -0.0921, -0.1886, -1.4833, 1.4104, 2.8937) ['Ά', 'ма', '↳', 'blatt', '⇔', '∘', '{`', 'temp', 'ℓ', 'чи']\n",
      "('30_5933', 0.0923, 0.0727, -0.0197, 1.1264, 0.8947, -0.2317) ['eight', '8', 'VIII', 'huit', 'acht', '⁸', 'XVIII', '₈', '八', 'eigh']\n",
      "('28_7844', 0.0886, 0.0519, -0.0367, 0.8304, 0.4865, -0.3439) ['eight', '8', '⁸', 'VIII', 'huit', 'acht', '₈', '八', 'eigh', 'XVIII']\n",
      "('30_6171', 0.0818, 0.0849, 0.0031, -2.5899, -2.6838, -0.0939) ['eight', 'VIII', '₈', '⁸', 'ré', '8', 'Mik', 'mongodb', 'Ё', 'Ris']\n"
     ]
    }
   ],
   "source": [
    "#identifying the deep FFN neurons using comparable neuron analysis (CNA) method\n",
    "all_ffn_subvalues = []\n",
    "for layer_i in range(LAYER_NUM):\n",
    "    coefficient_scores = torch.tensor(all_pos_coefficient_scores[layer_i][-1])\n",
    "    fc2_vectors = get_fc2_params(model, layer_i)\n",
    "    ffn_subvalues = (coefficient_scores * fc2_vectors).T\n",
    "    all_ffn_subvalues.append(ffn_subvalues)\n",
    "all_ffn_subvalues1 = []\n",
    "for layer_i in range(LAYER_NUM):\n",
    "    coefficient_scores1 = torch.tensor(all_pos_coefficient_scores1[layer_i][-1])\n",
    "    fc2_vectors1 = get_fc2_params(model1, layer_i)\n",
    "    ffn_subvalues1 = (coefficient_scores1 * fc2_vectors1).T\n",
    "    all_ffn_subvalues1.append(ffn_subvalues1)\n",
    "all_ffn_log_increase = get_log_increase(model, all_ffn_subvalues, all_pos_residual_output, final_var, predict_index)\n",
    "all_ffn_log_increase1 = get_log_increase(model1, all_ffn_subvalues1, all_pos_residual_output1, final_var, predict_index)\n",
    "all_ffn_scores = []\n",
    "for layer_i in range(LAYER_NUM):\n",
    "    for neuron_index in range(len(all_ffn_subvalues[0])):\n",
    "        all_ffn_scores.append((str(layer_i)+\"_\"+str(neuron_index), \n",
    "                               round(all_ffn_log_increase[layer_i][neuron_index] - all_ffn_log_increase1[layer_i][neuron_index], 4), \n",
    "                               round(all_ffn_log_increase[layer_i][neuron_index], 4), \n",
    "                               round(all_ffn_log_increase1[layer_i][neuron_index], 4), \n",
    "                               round(all_pos_coefficient_scores[layer_i][-1][neuron_index]-all_pos_coefficient_scores1[layer_i][-1][neuron_index], 4), \n",
    "                               round(all_pos_coefficient_scores[layer_i][-1][neuron_index], 4), \n",
    "                               round(all_pos_coefficient_scores1[layer_i][-1][neuron_index], 4)))\n",
    "all_ffn_scores_sort = sorted(all_ffn_scores, key=lambda x: x[1])[::-1]\n",
    "for x in all_ffn_scores_sort[:10]:\n",
    "    layer = int(x[0].split(\"_\")[0])\n",
    "    neuron = int(x[0].split(\"_\")[1])\n",
    "    cur_embed = all_ffn_subvalues[layer][neuron]\n",
    "    cur_embed_bsvalue = get_bsvalues(cur_embed, model, final_var)\n",
    "    cur_embed_bsvalue_sort = torch.argsort(cur_embed_bsvalue, descending=True)\n",
    "    print(x, [tokenizer.decode(a) for a in cur_embed_bsvalue_sort[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07df359-b3a2-464b-b622-30cafbbfafbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase:  0.2040548324584961 coefficient score:  3.790210723876953\n",
      "top value:  ['eight', 'VIII', '⁸', '8', 'III', 'huit', '₈', 'acht', 'XVIII', '<0x88>']\n",
      "last value:  ['io', 'Vier', 'ied', 'ilo', 'fourth', 'Four', '四', 'ier', 'ismo', 'nin']\n",
      "up score:  2.997763156890869 gate score:  1.7075436115264893\n",
      "up symbol:  1.0 gate symbol:  1.0\n"
     ]
    }
   ],
   "source": [
    "#compute the coefficient score of the selected FFN neuron (layer 19, neuron 5769)\n",
    "test_layer_ffn, test_index_ffn = 19, 5769\n",
    "print(\"increase: \", all_ffn_log_increase[test_layer_ffn][test_index_ffn], \n",
    "      \"coefficient score: \", torch.tensor(all_pos_coefficient_scores[test_layer_ffn][-1])[test_index_ffn].item())\n",
    "fc2_vector = get_fc2_params(model, test_layer_ffn).T[test_index_ffn]\n",
    "fc2_vector_bsvalue = get_bsvalues(fc2_vector, model, final_var)\n",
    "fc2_vector_bsvalue_sort = torch.argsort(fc2_vector_bsvalue, descending=True)\n",
    "print(\"top value: \", [tokenizer.decode(x) for x in fc2_vector_bsvalue_sort[:10]])\n",
    "print(\"last value: \", [tokenizer.decode(x) for x in fc2_vector_bsvalue_sort.tolist()[::-1][:10]])\n",
    "fc1_vector_up = model.model.layers[test_layer_ffn].mlp.up_proj.weight.data[test_index_ffn].data\n",
    "fc1_vector_gate = model.model.layers[test_layer_ffn].mlp.gate_proj.weight.data[test_index_ffn].data\n",
    "cur_layernorm_weight = get_layernorm_weight(model, test_layer_ffn)\n",
    "cur_ffn_key_up = cur_layernorm_weight * fc1_vector_up\n",
    "cur_ffn_key_gate = cur_layernorm_weight * fc1_vector_gate\n",
    "up_score_all = torch.sum(torch.tensor(all_pos_residual_output[test_layer_ffn][-1]) * cur_ffn_key_up).item()\n",
    "gate_score_all = torch.sum(torch.tensor(all_pos_residual_output[test_layer_ffn][-1]) * cur_ffn_key_gate).item()\n",
    "up_sym = 1.0 if up_score_all >= 0.0 else -1.0\n",
    "gate_sym = 1.0 if gate_score_all >= 0.0 else -1.0\n",
    "print(\"up score: \", up_score_all, \"gate score: \", gate_score_all)\n",
    "print(\"up symbol: \", up_sym, \"gate symbol: \", gate_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c610d632-a68a-4108-b394-043bcf367eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 up:  0.019892985001206398 gate:  0.128106951713562\n",
      "input ['<s>', 'sime', 'SERT', 'bolds', 'multicol', 'engelsk', 'UIView', 'ALSE', 'Ľ', 'kaf']\n",
      "subvalue ['', '<0x0A>', ',', '(', '-', '.', 'of', '\\xa0', ':', 's']\n",
      "1 up:  -0.006079650484025478 gate:  -0.005465351976454258\n",
      "input ['\\ufeff', '←', '<0xE2>', 'ℚ', '\\u200b', '1', 'Â', '��', 'ï', '\\xad']\n",
      "subvalue ['nom', 'nom', 'cat', 'ULT', 'cat', 'Nom', 'mans', 'ten', 'ход', 'Cat']\n",
      "2 up:  -0.03674646466970444 gate:  -0.028093719854950905\n",
      "input ['rd', 'thoughts', 'D', '0', 'new', 'things', 'bed', '6', 'M', 'Sister']\n",
      "subvalue ['fourth', '⁴', '4', 'YY', 'uchar', 'papers', 'aterra', 'apers', 'ASE', 'UNION']\n",
      "3 up:  -0.01766836829483509 gate:  0.011767487972974777\n",
      "input ['/-', 'Sample', 'hours', 'hour', 'years', '-+', 'illa', '⁄', 'hour', 'Cleveland']\n",
      "subvalue ['minus', '(-', '(-', 'között', 'minus', 'esser', 'onto', '++', 'üll', 'uno']\n",
      "4 up:  0.05119187384843826 gate:  0.06610628962516785\n",
      "input ['+', 'rule', 'dual', '=', '″', '=', 'aci', 'yr', 'Chair', 'hp']\n",
      "subvalue ['Ehr', 'Lane', 'unic', 'Errors', '<0xB6>', 'umeric', 'sixth', 'typeof', 'neur', 'Six']\n",
      "5 up:  0.5449588894844055 gate:  0.08487918972969055\n",
      "input ['totalité', 'мой', 'together', 'both', 'óm', 'Mate', 'configurations', 'Configuration', 'HT', 'sum']\n",
      "subvalue ['<s>', 'transformation', 'det', 'arc', 'cont', 'Tamb', 'DOC', 'ケ', 'Laurent', 'Run']\n"
     ]
    }
   ],
   "source": [
    "#analyze layer 17\n",
    "#the last position has the largest inner product with the subkey of the selected deep FFN neuron\n",
    "attn_test_layer = 17\n",
    "test_layer_input = torch.tensor(all_pos_layer_input[attn_test_layer])\n",
    "test_layer_input_bsvalues = get_bsvalues(test_layer_input, model, final_var)\n",
    "test_layer_input_sort = torch.argsort(test_layer_input_bsvalues, descending=True)\n",
    "cur_v_heads = torch.tensor(all_last_attn_subvalues[attn_test_layer])\n",
    "cur_attn_o_split = model.model.layers[attn_test_layer].self_attn.o_proj.weight.data.T.view(HEAD_NUM, HEAD_DIM, -1)\n",
    "cur_attn_subvalues_headrecompute = torch.bmm(cur_v_heads, cur_attn_o_split).permute(1, 0, 2)\n",
    "test_layer_subvalues = torch.sum(cur_attn_subvalues_headrecompute, 1)\n",
    "test_layer_subvalues_bsvalues = get_bsvalues(test_layer_subvalues, model, final_var)\n",
    "test_layer_subvalues_sort = torch.argsort(test_layer_subvalues_bsvalues, descending=True)\n",
    "attn_inner_products_up = torch.sum(test_layer_subvalues*cur_ffn_key_up, -1)\n",
    "attn_inner_products_gate = torch.sum(test_layer_subvalues*cur_ffn_key_gate, -1)\n",
    "for pos in range(len(test_layer_input_sort)):\n",
    "    print(pos, \"up: \", attn_inner_products_up[pos].item(), \"gate: \", attn_inner_products_gate[pos].item())\n",
    "    print(\"input\", [tokenizer.decode(x) for x in test_layer_input_sort[pos][:10]])\n",
    "    print(\"subvalue\", [tokenizer.decode(x) for x in test_layer_subvalues_sort[pos][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27056883-8e04-4a2e-a9f8-23c0b853c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head:  22 up_score:  0.4484 gate_score:  0.2736\n",
      "head:  10 up_score:  0.0052 gate_score:  0.0079\n",
      "head:  27 up_score:  0.004 gate_score:  0.0052\n",
      "head:  7 up_score:  -0.0011 gate_score:  0.0103\n",
      "head:  25 up_score:  0.0071 gate_score:  0.0014\n",
      "head:  23 up_score:  0.0065 gate_score:  0.0004\n",
      "head:  1 up_score:  0.0031 gate_score:  0.0025\n",
      "head:  9 up_score:  0.0055 gate_score:  -0.0022\n",
      "head:  15 up_score:  0.0036 gate_score:  -0.0018\n",
      "head:  30 up_score:  -0.0014 gate_score:  0.003\n",
      "head:  26 up_score:  -0.0003 gate_score:  0.0016\n",
      "head:  29 up_score:  0.0009 gate_score:  0.0004\n",
      "head:  14 up_score:  0.0007 gate_score:  0.0004\n",
      "head:  28 up_score:  0.0005 gate_score:  0.0001\n",
      "head:  11 up_score:  0.0005 gate_score:  -0.0002\n",
      "head:  3 up_score:  -0.0031 gate_score:  0.0032\n",
      "head:  24 up_score:  0.0001 gate_score:  -0.0001\n",
      "head:  2 up_score:  -0.0001 gate_score:  -0.0\n",
      "head:  16 up_score:  0.0006 gate_score:  -0.0008\n",
      "head:  5 up_score:  -0.0018 gate_score:  0.0015\n",
      "head:  13 up_score:  -0.0001 gate_score:  -0.0012\n",
      "head:  0 up_score:  -0.0001 gate_score:  -0.0017\n",
      "head:  17 up_score:  0.0014 gate_score:  -0.0034\n",
      "head:  6 up_score:  -0.0008 gate_score:  -0.0016\n",
      "head:  19 up_score:  -0.0049 gate_score:  0.002\n",
      "head:  31 up_score:  -0.0015 gate_score:  -0.0024\n",
      "head:  12 up_score:  0.0075 gate_score:  -0.0118\n",
      "head:  4 up_score:  0.0156 gate_score:  -0.0219\n",
      "head:  18 up_score:  -0.001 gate_score:  -0.0079\n",
      "head:  21 up_score:  -0.0079 gate_score:  -0.0046\n",
      "head:  20 up_score:  0.0068 gate_score:  -0.0241\n",
      "head:  8 up_score:  0.0513 gate_score:  -0.1427\n"
     ]
    }
   ],
   "source": [
    "#compute head score on the last position: head 22 is the most important head in layer 17\n",
    "test_pos = 5\n",
    "cur_layer_input = torch.tensor(all_pos_layer_input[attn_test_layer])\n",
    "cur_v_heads = torch.tensor(all_last_attn_subvalues[attn_test_layer])\n",
    "cur_attn_o_split = model.model.layers[attn_test_layer].self_attn.o_proj.weight.data.T.view(HEAD_NUM, HEAD_DIM, -1)\n",
    "cur_attn_subvalues_headrecompute = torch.bmm(cur_v_heads, cur_attn_o_split).permute(1, 0, 2)\n",
    "cur_attn_subvalues_head_curpos = cur_attn_subvalues_headrecompute[test_pos]\n",
    "cur_layer_input_last = cur_layer_input[-1]\n",
    "origin_prob = torch.log(get_prob(get_bsvalues(cur_layer_input_last, model, final_var))[predict_index])\n",
    "cur_attn_subvalues_head_plus = cur_attn_subvalues_head_curpos + cur_layer_input_last\n",
    "cur_attn_plus_probs = torch.log(get_prob(get_bsvalues(\n",
    "    cur_attn_subvalues_head_plus, model, final_var))[:, predict_index])\n",
    "cur_attn_plus_probs_increase = cur_attn_plus_probs - origin_prob\n",
    "cur_pos_heads_inner_products_up = torch.sum(cur_attn_subvalues_head_curpos * cur_ffn_key_up, -1)\n",
    "cur_pos_heads_inner_products_gate = torch.sum(cur_attn_subvalues_head_curpos * cur_ffn_key_gate, -1)\n",
    "cur_attn_plus_probs_increase_zip = []\n",
    "for i in range(len(cur_attn_plus_probs_increase)):\n",
    "    cur_attn_plus_probs_increase_zip.append((i, round(cur_attn_plus_probs_increase[i].item(), 4), \n",
    "                                             round(cur_pos_heads_inner_products_up[i].item(), 4), \n",
    "                                             round(cur_pos_heads_inner_products_gate[i].item(), 4)))\n",
    "cur_attn_plus_probs_increase_sort = sorted(cur_attn_plus_probs_increase_zip, key=lambda x: x[3]+x[2])[::-1]\n",
    "for head_index0, _, up_score, gate_score in cur_attn_plus_probs_increase_sort:\n",
    "    print(\"head: \", head_index0, \"up_score: \", up_score, \"gate_score: \", gate_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec77710d-9d5c-4bce-958f-b35f1bbfc0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15.0, (0.12116342782974243, 0.0976787805557251)), (13.0, (0.09483273327350616, 0.09762558341026306)), (14.0, (0.052968062460422516, 0.013312157243490219)), (8.0, (0.023074448108673096, -0.005840588361024857)), (7.0, (0.019325416535139084, 0.015491408295929432)), (6.0, (0.019316932186484337, -0.009192920289933681)), (11.0, (0.019167589023709297, -0.008673514239490032)), (16.5, (0.019051995128393173, -0.006628106348216534)), (7.5, (0.018388744443655014, -0.011169372126460075)), (15.5, (0.0173991359770298, 0.0006903298199176788))]\n"
     ]
    }
   ],
   "source": [
    "#calculate which layer-level vector is the most important in head 17_22's last position's vector\n",
    "pos_embed_var = torch.tensor(all_pos_layer_input)[attn_test_layer][test_pos].pow(2).mean(-1, keepdim=True)\n",
    "curhead_v = model.model.layers[attn_test_layer].self_attn.v_proj.weight.split(HEAD_DIM)[head_index]\n",
    "curhead_o = model.model.layers[attn_test_layer].self_attn.o_proj.weight.T.split(HEAD_DIM)[head_index]\n",
    "previous_vectors = [torch.tensor(all_pos_layer_input)[0][test_pos]]\n",
    "for layer_i in range(attn_test_layer):\n",
    "    previous_vectors.append(torch.tensor(all_pos_attn_output)[layer_i][test_pos])\n",
    "    previous_vectors.append(torch.tensor(all_pos_ffn_output)[layer_i][test_pos])\n",
    "all_scores, all_o = [], []\n",
    "for i, embed in enumerate(previous_vectors):\n",
    "    embed = get_pos_vector(embed, pos_embed_var, model, attn_test_layer)\n",
    "    cur_embed_v = torch.sum(curhead_v * embed, 1)\n",
    "    cur_embed_o = torch.sum(curhead_o.T * cur_embed_v, 1)\n",
    "    all_o.append(cur_embed_o)\n",
    "    cur_embed_inner_products_up = torch.sum(cur_embed_o * cur_ffn_key_up, -1)\n",
    "    cur_embed_inner_products_gate = torch.sum(cur_embed_o * cur_ffn_key_gate, -1)\n",
    "    all_scores.append((cur_embed_inner_products_up.item(), cur_embed_inner_products_gate.item()))\n",
    "all_scores_sort = sorted(zip([i/2-0.5 for i in range(len(all_scores))], all_scores), key=lambda x: x[1][0])[::-1]\n",
    "print(all_scores_sort[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b5ba27-fe13-475b-b68e-4be3ad1b57b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos:  0 up:  -0.0012112583499401808 gate:  0.0016715569654479623\n",
      "pos:  1 up:  -0.0005289626424200833 gate:  -0.000134551664814353\n",
      "pos:  2 up:  0.06307642161846161 gate:  0.09762759506702423\n",
      "pos:  3 up:  0.014674997888505459 gate:  0.0052227722480893135\n",
      "pos:  4 up:  0.0697443038225174 gate:  0.006158954929560423\n",
      "pos:  5 up:  -0.024592075496912003 gate:  -0.012867532670497894\n"
     ]
    }
   ],
   "source": [
    "#see which previous vector at which position is useful (find previous subvalue's pos)\n",
    "previous_layer = 15\n",
    "cur_v_heads = torch.tensor(all_last_attn_subvalues[previous_layer])\n",
    "cur_attn_o_split = model.model.layers[previous_layer].self_attn.o_proj.weight.data.T.view(HEAD_NUM, HEAD_DIM, -1)\n",
    "cur_attn_subvalues_headrecompute = torch.bmm(cur_v_heads, cur_attn_o_split).permute(1, 0, 2)\n",
    "previous_layer_attnvalues = torch.sum(cur_attn_subvalues_headrecompute, 1)\n",
    "for pos in range(len(previous_layer_attnvalues)):\n",
    "    cur_embed = previous_layer_attnvalues[pos]\n",
    "    cur_embed_bsvalue = get_bsvalues(cur_embed, model, final_var)\n",
    "    cur_embed_bsvalue_sort = torch.argsort(cur_embed_bsvalue, descending=True)\n",
    "    cur_embed_ln = get_pos_vector(cur_embed, pos_embed_var, model, attn_test_layer)\n",
    "    cur_embed_v = torch.sum(curhead_v * cur_embed_ln, 1)\n",
    "    cur_embed_o = torch.sum(curhead_o.T * cur_embed_v, 1)\n",
    "    cur_embed_inner_products_up = torch.sum(cur_embed_o * cur_ffn_key_up, -1)\n",
    "    cur_embed_inner_products_gate = torch.sum(cur_embed_o * cur_ffn_key_gate, -1)\n",
    "    print(\"pos: \", pos, \"up: \", cur_embed_inner_products_up.item(), \"gate: \", cur_embed_inner_products_gate.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d523fbcf-f79d-40f3-99b6-630edf107a1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12.5, 0.03383322432637215, 0.07460989058017731), (10.5, 0.019165528938174248, 0.015703681856393814), (14.5, 0.01742423139512539, 0.00963421631604433), (6.5, 0.016220010817050934, 0.02717570960521698), (0.5, 0.01072070561349392, 0.01012455578893423), (1.5, 0.010294288396835327, 0.01426645927131176), (8.0, 0.009111599996685982, 0.00042461464181542397), (11.0, 0.007322901394218206, 0.009814698249101639), (-0.5, 0.006614874117076397, 0.010071543976664543), (12.0, 0.006437157280743122, -0.008164125494658947)]\n"
     ]
    }
   ],
   "source": [
    "#see which previous layer is useful for computing attn subvalue 15_2\n",
    "mask_layer, mask_pos = 15, 2\n",
    "test_embed1 = torch.tensor(all_pos_layer_input[0][mask_pos])\n",
    "pos_embed_var1_mask_layer = torch.tensor(all_pos_layer_input[mask_layer][mask_pos]).pow(2).mean(-1, keepdim=True)\n",
    "previous_masklayer = [torch.tensor(all_pos_layer_input[0][mask_pos])]\n",
    "for i in range(mask_layer):\n",
    "    previous_masklayer.append(torch.tensor(all_pos_attn_output[i][mask_pos]))\n",
    "    previous_masklayer.append(torch.tensor(all_pos_ffn_output[i][mask_pos]))\n",
    "\n",
    "all_previous_vo = []\n",
    "for i, embed in enumerate(previous_masklayer):\n",
    "    embed_ln = get_pos_vector(embed, pos_embed_var1_mask_layer, model, mask_layer)\n",
    "    embed_ov = model.model.layers[mask_layer].self_attn.o_proj(model.model.layers[mask_layer].self_attn.v_proj(embed_ln))\n",
    "    embed_ln2 = get_pos_vector(embed_ov, pos_embed_var, model, attn_test_layer)\n",
    "    embed_v2 = torch.sum(curhead_v * embed_ln2, 1)\n",
    "    embed_o2 = torch.sum(curhead_o.T * embed_v2, 1)\n",
    "    all_previous_vo.append(embed_o2)\n",
    "\n",
    "all_previous_vo_scores = []\n",
    "for i, x in enumerate(all_previous_vo):\n",
    "    all_previous_vo_scores.append((i/2-0.5, torch.sum(x * cur_ffn_key_up, -1).item(), torch.sum(x * cur_ffn_key_gate, -1).item()))\n",
    "\n",
    "all_previous_vo_scores_sort = sorted(all_previous_vo_scores, key=lambda x: x[1])[::-1]\n",
    "print(all_previous_vo_scores_sort[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f0d5f9-285d-4f60-8b4b-6c6dad710758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072 0.02357804775238037 0.055049389600753784\n",
      "value original:  ['rd', 'quarters', 'PO', 'peat', '⅓', 'Constraint', 'ran', 'avas', 'ր', 'angol']\n",
      "value transform:  ['III', 'three', 'Three', '三', '<0x84>', '₃', '3', 'three', 'Three', 'triple']\n"
     ]
    }
   ],
   "source": [
    "#see ffn subvalues on pos 4\n",
    "pos4_useful_layer_ffn = 12\n",
    "pos4_useful_layer_ffn_coeffs = torch.tensor(all_pos_coefficient_scores)[pos4_useful_layer_ffn][mask_pos]\n",
    "pos4_useful_layer_fc2 = get_fc2_params(model, pos4_useful_layer_ffn)\n",
    "pos4_useful_layer_ffn_subvalues = (pos4_useful_layer_ffn_coeffs * pos4_useful_layer_fc2).T\n",
    "pos4_useful_layer_ffn_subvalues_ln = get_pos_vector(pos4_useful_layer_ffn_subvalues, pos_embed_var1_mask_layer, model, mask_layer)\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov = model.model.layers[mask_layer].self_attn.o_proj(model.model.layers[mask_layer].self_attn.v_proj(pos4_useful_layer_ffn_subvalues_ln))\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov_ln2 = get_pos_vector(pos4_useful_layer_ffn_subvalues_ln_ov, pos_embed_var, model, attn_test_layer)\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2 = torch.bmm(pos4_useful_layer_ffn_subvalues_ln_ov_ln2.unsqueeze(0), curhead_v.T.unsqueeze(0))\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2_o2 = torch.bmm(pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2, curhead_o.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "all_ffn_pos_scores_up = torch.sum(pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2_o2 * cur_ffn_key_up, 1)\n",
    "all_ffn_pos_scores_gate = torch.sum(pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2_o2 * cur_ffn_key_gate, 1)\n",
    "all_ffn_pos_scores_upgate = []\n",
    "for i in range(len(all_ffn_pos_scores_gate)):\n",
    "    all_ffn_pos_scores_upgate.append((i, all_ffn_pos_scores_up[i].item(), all_ffn_pos_scores_gate[i].item()))\n",
    "\n",
    "all_ffn_pos_scores_upgate_sort = sorted(all_ffn_pos_scores_upgate, key=lambda x: x[1]+x[2])[::-1]\n",
    "for index, up_score, gate_score in all_ffn_pos_scores_upgate_sort[:1]:\n",
    "    print(index, up_score, gate_score)\n",
    "    embed = pos4_useful_layer_ffn_subvalues[index]\n",
    "    embed_bsvalue = get_bsvalues(embed, model, final_var)\n",
    "    embed_bsvalue_sort = torch.argsort(embed_bsvalue, descending=True)\n",
    "    print(\"value original: \", [tokenizer.decode(x) for x in embed_bsvalue_sort[:10]])\n",
    "    embed_ln = get_pos_vector(embed, pos_embed_var1_mask_layer, model, mask_layer)\n",
    "    embed_ov = model.model.layers[mask_layer].self_attn.o_proj(model.model.layers[mask_layer].self_attn.v_proj(embed_ln))\n",
    "    embed_bsvalue = get_bsvalues(embed_ov, model, final_var)\n",
    "    embed_bsvalue_sort = torch.argsort(embed_bsvalue, descending=True)\n",
    "    print(\"value transform: \", [tokenizer.decode(x) for x in embed_bsvalue_sort[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7529c-25e9-440d-9aab-ac50bea0d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8ba83-98d9-4430-a7c7-3aaa3b463632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "436c29d9-a659-4bde-a38a-f4d20b467119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14.5, 0.06068086624145508, 0.00012453319504857063), (11.5, 0.05477098375558853, 0.020668024197220802), (5.5, 0.0283675380051136, 0.006406363565474749), (7.5, 0.02060963399708271, 0.02500823885202408), (12.5, 0.017112277448177338, 0.04150316119194031), (0.5, 0.012670803815126419, 0.008029316551983356), (14.0, 0.01023088488727808, -0.012319373898208141), (9.0, 0.009961117058992386, -0.0029868069104850292), (2.5, 0.008939795196056366, -0.0016081184148788452), (11.0, 0.008081584237515926, 0.009449582546949387)]\n"
     ]
    }
   ],
   "source": [
    "#see which previous layer is useful for computing attn subvalue 13_4\n",
    "mask_layer, mask_pos = 15, 4\n",
    "test_embed1 = torch.tensor(all_pos_layer_input[0][mask_pos])\n",
    "pos_embed_var1_mask_layer = torch.tensor(all_pos_layer_input[mask_layer][mask_pos]).pow(2).mean(-1, keepdim=True)\n",
    "previous_masklayer = [torch.tensor(all_pos_layer_input[0][mask_pos])]\n",
    "for i in range(mask_layer):\n",
    "    previous_masklayer.append(torch.tensor(all_pos_attn_output[i][mask_pos]))\n",
    "    previous_masklayer.append(torch.tensor(all_pos_ffn_output[i][mask_pos]))\n",
    "\n",
    "all_previous_vo = []\n",
    "for i, embed in enumerate(previous_masklayer):\n",
    "    embed_ln = get_pos_vector(embed, pos_embed_var1_mask_layer, model, mask_layer)\n",
    "    embed_ov = model.model.layers[mask_layer].self_attn.o_proj(model.model.layers[mask_layer].self_attn.v_proj(embed_ln))\n",
    "    embed_ln2 = get_pos_vector(embed_ov, pos_embed_var, model, attn_test_layer)\n",
    "    embed_v2 = torch.sum(curhead_v * embed_ln2, 1)\n",
    "    embed_o2 = torch.sum(curhead_o.T * embed_v2, 1)\n",
    "    all_previous_vo.append(embed_o2)\n",
    "\n",
    "all_previous_vo_scores = []\n",
    "for i, x in enumerate(all_previous_vo):\n",
    "    all_previous_vo_scores.append((i/2-0.5, torch.sum(x * cur_ffn_key_up, -1).item(), torch.sum(x * cur_ffn_key_gate, -1).item()))\n",
    "\n",
    "all_previous_vo_scores_sort = sorted(all_previous_vo_scores, key=lambda x: x[1])[::-1]\n",
    "print(all_previous_vo_scores_sort[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f7cdeb9-e508-4acb-85c3-28e9e049989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2258 0.02762100100517273 0.012637176550924778\n",
      "value original:  ['ös', 'enz', 'Trace', 'lis', 'vid', 'suite', 'HT', 'ung', 'ane', 'icano']\n",
      "value transform:  ['XV', 'fifth', 'Fif', 'avas', 'Five', 'five', 'abase', '五', '₅', 'fif']\n"
     ]
    }
   ],
   "source": [
    "#see ffn subvalues on pos 4\n",
    "pos4_useful_layer_ffn = 11\n",
    "pos4_useful_layer_ffn_coeffs = torch.tensor(all_pos_coefficient_scores)[pos4_useful_layer_ffn][mask_pos]\n",
    "pos4_useful_layer_fc2 = get_fc2_params(model, pos4_useful_layer_ffn)\n",
    "pos4_useful_layer_ffn_subvalues = (pos4_useful_layer_ffn_coeffs * pos4_useful_layer_fc2).T\n",
    "pos4_useful_layer_ffn_subvalues_ln = get_pos_vector(pos4_useful_layer_ffn_subvalues, pos_embed_var1_mask_layer, model, mask_layer)\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov = model.model.layers[mask_layer].self_attn.o_proj(model.model.layers[mask_layer].self_attn.v_proj(pos4_useful_layer_ffn_subvalues_ln))\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov_ln2 = get_pos_vector(pos4_useful_layer_ffn_subvalues_ln_ov, pos_embed_var, model, attn_test_layer)\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2 = torch.bmm(pos4_useful_layer_ffn_subvalues_ln_ov_ln2.unsqueeze(0), curhead_v.T.unsqueeze(0))\n",
    "pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2_o2 = torch.bmm(pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2, curhead_o.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "all_ffn_pos_scores_up = torch.sum(pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2_o2 * cur_ffn_key_up, 1)\n",
    "all_ffn_pos_scores_gate = torch.sum(pos4_useful_layer_ffn_subvalues_ln_ov_ln2_v2_o2 * cur_ffn_key_gate, 1)\n",
    "all_ffn_pos_scores_upgate = []\n",
    "for i in range(len(all_ffn_pos_scores_gate)):\n",
    "    all_ffn_pos_scores_upgate.append((i, all_ffn_pos_scores_up[i].item(), all_ffn_pos_scores_gate[i].item()))\n",
    "\n",
    "all_ffn_pos_scores_upgate_sort = sorted(all_ffn_pos_scores_upgate, key=lambda x: x[1]+x[2])[::-1]\n",
    "for index, up_score, gate_score in all_ffn_pos_scores_upgate_sort[:1]:\n",
    "    print(index, up_score, gate_score)\n",
    "    embed = pos4_useful_layer_ffn_subvalues[index]\n",
    "    embed_bsvalue = get_bsvalues(embed, model, final_var)\n",
    "    embed_bsvalue_sort = torch.argsort(embed_bsvalue, descending=True)\n",
    "    print(\"value original: \", [tokenizer.decode(x) for x in embed_bsvalue_sort[:10]])\n",
    "    embed_ln = get_pos_vector(embed, pos_embed_var1_mask_layer, model, mask_layer)\n",
    "    embed_ov = model.model.layers[mask_layer].self_attn.o_proj(model.model.layers[mask_layer].self_attn.v_proj(embed_ln))\n",
    "    embed_bsvalue = get_bsvalues(embed_ov, model, final_var)\n",
    "    embed_bsvalue_sort = torch.argsort(embed_bsvalue, descending=True)\n",
    "    print(\"value transform: \", [tokenizer.decode(x) for x in embed_bsvalue_sort[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58af06-c9c8-4d56-ba3c-9896026655a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
